{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "play_char.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oic77yi0k2Z2"
      },
      "source": [
        "## Train a character-level GPT on some text data\n",
        "\n",
        "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBOCJjxxk_0F",
        "outputId": "16b0300b-3c2b-4798-be69-902fbb8cc75a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        }
      },
      "source": [
        "# Added by Marxav: import minGPT library (only 3 .py files) and import a dataset (input.txt file)\n",
        "!wget -N https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/model.py\n",
        "!wget -N https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/utils.py\n",
        "!wget -N https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/trainer.py\n",
        "!wget -N https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!mkdir mingpt\n",
        "!mv *.py mingpt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-08 10:02:19--  https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8455 (8.3K) [text/plain]\n",
            "Saving to: ‘model.py’\n",
            "\n",
            "\rmodel.py              0%[                    ]       0  --.-KB/s               \rmodel.py            100%[===================>]   8.26K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2020-10-08 10:02:19 (82.8 MB/s) - ‘model.py’ saved [8455/8455]\n",
            "\n",
            "--2020-10-08 10:02:20--  https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1718 (1.7K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]   1.68K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2020-10-08 10:02:20 (39.5 MB/s) - ‘utils.py’ saved [1718/1718]\n",
            "\n",
            "--2020-10-08 10:02:20--  https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/trainer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5268 (5.1K) [text/plain]\n",
            "Saving to: ‘trainer.py’\n",
            "\n",
            "trainer.py          100%[===================>]   5.14K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2020-10-08 10:02:20 (60.8 MB/s) - ‘trainer.py’ saved [5268/5268]\n",
            "\n",
            "--2020-10-08 10:02:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2020-10-08 10:02:20 (9.51 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "mkdir: cannot create directory ‘mingpt’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLrNh07sk2ay"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwkWURA5n9q9",
        "outputId": "61008df5-b84b-45d1-fc16-08bc01f15f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Added by Marxav: add info about the GPU\n",
        "print('device_name:', torch.cuda.get_device_name())\n",
        "print('device_count:', torch.cuda.device_count())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device_name: Tesla P4\n",
            "device_count: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUTCb0Rpk2aY"
      },
      "source": [
        "# make deterministic\n",
        "from mingpt.utils import set_seed\n",
        "set_seed(42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqHzn2T7k2bC"
      },
      "source": [
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        \"\"\"\n",
        "        arrange data and targets so that the first i elements of x\n",
        "        will be asked to predict the i-th element of y. Notice that\n",
        "        the eventual language model will actually make block_size\n",
        "        individual predictions at the same time based on this data,\n",
        "        so we are being clever and amortizing the cost of the forward\n",
        "        pass of the network. So for example if block_size is 4, then\n",
        "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
        "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
        "        then actually \"multitask\" 4 separate examples at the same time\n",
        "        in the language model:\n",
        "        - given just \"h\", please predict \"e\" as next\n",
        "        - given \"he\" please predict \"l\" next\n",
        "        - given \"hel\" predict \"l\" next\n",
        "        - given \"hell\" predict \"o\" next\n",
        "        \n",
        "        In addition, because the DataLoader will create batches of examples,\n",
        "        every forward/backward pass during traning will simultaneously train\n",
        "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
        "        for a batched input of integers X (B, T) where B is batch size and\n",
        "        T is block_size and Y (B, T), the network will during training be\n",
        "        simultaneously training to make B*T predictions, all at once! Of course,\n",
        "        at test time we can paralellize across batch B, but unlike during training\n",
        "        we cannot parallelize across the time dimension T - we have to run\n",
        "        a forward pass of the network to recover the next single character of the \n",
        "        sequence along each batch dimension, and repeatedly always feed in a next\n",
        "        character to get the next one.\n",
        "        \n",
        "        So yes there is a big asymmetry between train/test time of autoregressive\n",
        "        models. During training we can go B*T at a time with every forward pass,\n",
        "        but during test time we can only go B at a time, T times, with T forward \n",
        "        passes.\n",
        "        \"\"\"\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59sEiPMVk2bL"
      },
      "source": [
        "block_size = 128 # spatial extent of the model for its context"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsE6-Mn-k2bX",
        "outputId": "2bc8530b-0e74-468b-ec5c-76931655bf27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "text = text[0:200000] # downsized by XM for a 1-minute demo\n",
        "train_dataset = CharDataset(text[0:200000], block_size) # one line of poem is roughly 50 characters"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 200000 characters, 62 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALHg0R2kk2bl"
      },
      "source": [
        "from mingpt.model import GPT, GPTConfig\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
        "                  n_layer=2, n_head=2, n_embd=128)\n",
        "                  #n_layer=8, n_head=8, n_embd=512) # downsized by XM for a 1-minute demo\n",
        "model = GPT(mconf)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clUpfbEGk2by",
        "outputId": "3931f540-53c2-4b73-b5d1-b06fa2795f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "#tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4, # downsized by XM for a 1-minute demo\n",
        "tconf = TrainerConfig(max_epochs=1, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
        "                      #num_workers=4) # downsized by XM for a 1-minute demo\n",
        "                      num_workers=1)\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\n",
        "trainer.train()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1 iter 390: train loss 1.92737. lr 3.000943e-04: 100%|██████████| 391/391 [01:17<00:00,  5.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asaLfLJvk2b7",
        "outputId": "e71206dd-a8ea-46bf-8318-c1226183a66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# alright, let's sample some character-level Shakespeare\n",
        "from mingpt.utils import sample\n",
        "\n",
        "context = \"O God, O God!\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O God, O God!\n",
            "COMINIUS:\n",
            "What's me had be ceturys seats motch the dows. Than the sow, tursts,-\n",
            "He cas he the back; faten sand the have arge se will. My, helch come have sen that mis the hou pone.\n",
            "\n",
            "Them he when so, the peer: not to he brught. I think mee my will to shath beers,\n",
            "Thalt thes: sto he word all bearrt his in at then but o of you,\n",
            "Your that apppire, son feeace\n",
            "Wen true to of my beant the word, both he sute at bear their is for\n",
            "Made thir that, the think and word o mell bether.\n",
            "Out wath what words your have the coms, a wich he cof hadsere thy pace\n",
            "Thim, are's swere counss maccis,.\n",
            "Second Say shim mes ase warch as ortthe boody thy\n",
            "Trrught sprices, wall say that with that in he and the have\n",
            "Auffir the ince hey, the to they pack ser,\n",
            "Whes in shild th hom shall to an sto with cour of ine wen,\n",
            "Then brow, as warrd yee he prartionsster\n",
            "Sher brateng a fart my and is atend\n",
            "My complenes.\n",
            "\n",
            "SICINIUS:\n",
            "Noh, hing sit.\n",
            "\n",
            "SICINIUS:\n",
            "A wish.\n",
            "\n",
            "MENENIUS:\n",
            "Sto to\n",
            "\n",
            "Bet say sto wer tre or and harrk.\n",
            "\n",
            "Stizonss:\n",
            "The sourt to sa ther?\n",
            "CORIUS:\n",
            "Now,\n",
            "As net you was you th hip you the bees,\n",
            "To mate, morther his bood trumpone to hing!\n",
            "AUFIDIDIUS:\n",
            "My bries, a sto thim ouch in thang all wifence ars onsuld of mysst be and the be tis for a thilles, he\n",
            "The have ben swhat\n",
            "The saint we a and thiestay co of beich as to be of your muniry,\n",
            "To than swere ond this,\n",
            "As, and is fliesh amer, beast not my our of the the trit\n",
            "Ther bat theeir the sead in thie was a that's mand tas, an what is thang we all and\n",
            "The woure, the is thince. I hir inty the world\n",
            "You.\n",
            "\n",
            "\n",
            "AUFIDIUS:\n",
            "I hold your such wer merve on fair. Colsch,\n",
            "\n",
            "CORIOLANUS:\n",
            "That is with?\n",
            "\n",
            "LARTIUS:\n",
            "Take to the swith\n",
            "I the crter on the heading theeir the the prue the the warrtith hong that fors:; your ath, the wis he thre wuld now the to preer ine\n",
            "To to ser him ce.\n",
            "Whose coul be theear manis, an be als arg and his tand\n",
            "This the the strout\n",
            "Which opee the cul the would the he come\n",
            "Anto than to stand me he comble and ot the,\n",
            "Sthe sto ore the mors band him withe be the sa for\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc7Hk65Ck2cK"
      },
      "source": [
        "# well that was fun"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}